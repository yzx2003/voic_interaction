import os
import time
import json
import wave
import threading
import cv2
from PIL import Image 
import pygame
import pyaudio
import torch
import webrtcvad
import numpy as np
import re
from queue import Queue, Empty
from transformers import AutoProcessor
from qwen_vl_utils import process_vision_info 
from modelscope import AutoModel
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from transformers import Qwen2VLForConditionalGeneration
from transformers import BitsAndBytesConfig

import torch
torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
torch.backends.cudnn.benchmark = True

from scipy.signal import resample_poly 
from collections import deque

# é…ç½®å‚æ•°
AUDIO_CHANNELS = 1
AUDIO_RATE = 48000 
AUDIO_CHUNK = 1024
VAD_MODE = 3 #1 #2 #3
NO_SPEECH_THRESHOLD = 4
TTS_RATE = 150
TTS_VOLUME = 0.9
POINT_JSON_DIR = "point_json"
os.makedirs(POINT_JSON_DIR, exist_ok=True)
TASK_POINTS_FILE = os.path.join(POINT_JSON_DIR, "task_points.json")  # ä»»åŠ¡ç‚¹ä¿å­˜æ–‡ä»¶

# å‘½ä»¤ç±»å‹å®šä¹‰
class CommandType:
    A_TO_B = 1   # å¸¸è§„ä¸‰ä¸ªä»»åŠ¡ç‚¹
    B_TO_C = 2
    C_TO_D = 3
    D_TO_A = 4   # èµ·å§‹ç‚¹
    IMAGE_RECOGNITION = 5 # å›¾åƒæ£€æµ‹
    DIALOGUE = 6  # å¯¹è¯ç±»å‹
    ADD_TASK_POINT = 7  # æ·»åŠ ä»»åŠ¡ç‚¹
    MODIFY_TASK_POINT = 8  # ä¿®æ”¹ä»»åŠ¡ç‚¹
    NAVIGATE = 9  # åˆå¹¶å¯¼èˆªç±»å‹ï¼ˆåŒ…å«æŒ‰æè¿°å¯¼èˆªï¼‰
    GREET = 10  # æ‰“ä¸ªæ‹›å‘¼


class VoiceNavigationSystem:
    def __init__(self):
        # å‚æ•°è®¾ç½®
        self.mqtt_broker = '10.42.0.1' #æœ¬åœ° localhost  10.42.0.1
        self.mqtt_port = 1883
        self.mqtt_control_topic = 'robot_control'  # æ§åˆ¶æŒ‡ä»¤è¯é¢˜
        self.mqtt_status_topic = 'base_status'     # åº•ç›˜çŠ¶æ€è¯é¢˜
        self.qwen_model_path = '/home/y/LLM/Qwen2-VL-2B-Instruct'
        self.sensevoice_model_path = '/home/y/LLM/SenseVoiceSmall'
        print(f"ğŸ“Œ ä»»åŠ¡ç‚¹æ–‡ä»¶è·¯å¾„: {TASK_POINTS_FILE}")
        # è®¾å¤‡æ£€æµ‹ä¸åˆå§‹åŒ–
        self.device = None
        self._initialize_device()

        self.audio_stream = None
        self.pyaudio_instance = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.mqtt_client = None
        self.initialize_mqtt()
        self.tts_engine = None
        self.tts_lock = threading.Lock()
        self.command_queue = Queue()
        self.is_running = False
        self.is_listening = False
        self.FORMAT = pyaudio.paInt16
        self.tts_lock = threading.Lock()
        self.audio_block_lock = threading.Lock() 

        # éŸ³é¢‘è§†é¢‘é˜Ÿåˆ—
        self.audio_queue = Queue()
        self.video_queue = Queue(maxsize=100) # é™åˆ¶é˜Ÿåˆ—å¤§å°ï¼Œé¿å…é˜»å¡
        self.video_buffer = deque(maxlen=300)  # å­˜å‚¨10ç§’è§†é¢‘(æŒ‰30fps)
        self.last_active_time = time.time()
        self.recording_active = True
        self.segments_to_save = []
        self.saved_intervals = []
        self.last_vad_end_time = 0

        # åˆå§‹åŒ–VAD
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(VAD_MODE)
        
        # ä»»åŠ¡ç‚¹ç®¡ç†ï¼ˆID: {x, y, z, description}ï¼‰
        self.task_points = self._load_task_points()  # ä»æ–‡ä»¶åŠ è½½
        self.current_pose = None  # å­˜å‚¨å½“å‰åº•ç›˜ä½å§¿ (x, y, yaw)
        self.last_nav_command = None  # ä¿å­˜æœ€åä¸€ä¸ªå¯¼èˆªæŒ‡ä»¤
        self.last_nav_query = ""  # ä¿å­˜æœ€åä¸€ä¸ªå¯¼èˆªæŸ¥è¯¢æ–‡æœ¬

        self.is_cycling = False  # å¾ªç¯å¯¼èˆªæ ‡å¿—ä½
        self.current_cycle_step = 0  # å¾ªç¯æ­¥éª¤è®¡æ•°å™¨
        self.cycle_order = [1, 2, 3, 0]  # å¾ªç¯é¡ºåº
        self.completed_cycles = 0  # å·²å®Œæˆçš„ä»»åŠ¡å¾ªç¯æ¬¡æ•°
        self.max_cycles = 1  # æœ€å¤§ä»»åŠ¡å¾ªç¯æ¬¡æ•°ï¼ˆå›ºå®šä¸º1ï¼‰

        pygame.mixer.init()
        
        # å‘½ä»¤æ˜ å°„ï¼ˆåˆå¹¶å¯¼èˆªç›¸å…³æŒ‡ä»¤ï¼‰
        self.command_mapping = {
           
            # æ–°å¢ä»»åŠ¡ç‚¹æŒ‡ä»¤
            "æ·»åŠ ä»»åŠ¡ç‚¹": CommandType.ADD_TASK_POINT,
            "å¢åŠ ä»»åŠ¡ç‚¹": CommandType.ADD_TASK_POINT,
            "ä¿å­˜å½“å‰ç‚¹": CommandType.ADD_TASK_POINT,
            "è®°å½•è¿™ä¸ªç‚¹": CommandType.ADD_TASK_POINT,
            
            # ä¿®æ”¹ä»»åŠ¡ç‚¹æŒ‡ä»¤
            "è®¾å®šä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "è®¾ç½®ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "ç½®ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "è®¾ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "æ”¹ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "ä¿®æ”¹ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),
            "ä¸ºä¸€å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 1),

            "è®¾å®šä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "è®¾ç½®ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "è®¾ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "ç½®ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "æ”¹ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "ä¿®æ”¹ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),
            "ä¸ºäºŒå·ç‚¹": (CommandType.MODIFY_TASK_POINT, 2),

            "è®¾å®šä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "è®¾ç½®ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "ç½®ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "è®¾ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "æ”¹ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "ä¿®æ”¹ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),
            "ä¸ºä¸‰å·ç‚¹": (CommandType.MODIFY_TASK_POINT, 3),

            "è®¾å®šä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "è®¾ç½®ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "ç½®ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "è®¾ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "æ”¹ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "ä¿®æ”¹ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),
            "ä¸ºèµ·å§‹ç‚¹": (CommandType.MODIFY_TASK_POINT, 0),

            # æ‰“æ‹›å‘¼æŒ‡ä»¤æ˜ å°„
            "æ‰“ä¸ªæ‹›å‘¼": (CommandType.GREET, 9),
            "æ‰“ä¸ª": (CommandType.GREET, 9),
            "æ‹›å‘¼": (CommandType.GREET, 9),
            "é—®å€™ä¸€ä¸‹": (CommandType.GREET, 9),
            "é—®å€™": (CommandType.GREET, 9),
            "é—®å¥½": (CommandType.GREET, 9),
        }
        
        self.welcome_text = "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³å¯¼èˆªåŠ©æ‰‹ï¼Œè¯·è¯´å‡ºæ‚¨çš„æŒ‡ä»¤"
        self.listening_text = "æˆ‘åœ¨å¬ï¼Œè¯·è¯´è¯"

        print("ğŸ¤– æ™ºèƒ½è¯­éŸ³å¯¼èˆªç³»ç»Ÿåˆå§‹åŒ–ä¸­...")
        self.initialize_system()

    def _initialize_device(self):
        try:
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"CUDAå¯ç”¨ï¼Œä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
            else:
                self.device = torch.device("cpu")
                print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            test_tensor = torch.tensor([1.0], device=self.device)
            return True
        except Exception as e:
            print(f"è®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.device = torch.device("cpu")
            print("å¼ºåˆ¶ä½¿ç”¨CPUä½œä¸º fallback")
            return False

    def initialize_models(self):
        try:
            if self.device is None:
                self._initialize_device()
                
            # åŠ è½½åƒé—®VLæ¨¡å‹ï¼ˆç”¨äºå›¾åƒè¯†åˆ«å’Œåœºæ™¯æè¿°ï¼‰
            print("åŠ è½½åƒé—®VLæ¨¡å‹...")
            min_pixels = 128 * 28 * 28
            max_pixels = 256 * 28 * 28

            # 8bité‡åŒ–ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦ï¼‰
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_use_double_quant=True,
                bnb_8bit_compute_dype=torch.float16
            )
            self.qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.qwen_model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            self.qwen_model.eval()

            self.qwen_processor = AutoProcessor.from_pretrained(
                self.qwen_model_path,
                min_pixels=min_pixels,
                max_pixels=max_pixels,
                trust_remote_code=True
            )

            # åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹
            print("åŠ è½½SenseVoiceæ¨¡å‹...")
            self.sensevoice_pipeline = pipeline(
                task=Tasks.auto_speech_recognition,
                model=self.sensevoice_model_path,
                model_revision="v1.0.0",
                trust_remote_code=True
            )
            print("SenseVoiceæ¨¡å‹åŠ è½½æˆåŠŸ")
                
            print(f"æ‰€æœ‰æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ° {self.device}")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def initialize_mqtt(self):
        try:
            import paho.mqtt.client as mqtt
            self.mqtt_client = mqtt.Client(callback_api_version=mqtt.CallbackAPIVersion.VERSION2)
            self.mqtt_client.on_connect = self.on_connect
            self.mqtt_client.on_message = self.on_message
            self.mqtt_client.connect(self.mqtt_broker, self.mqtt_port, keepalive=60)
            self.mqtt_client.loop_start()
            return True
        except Exception as e:
            print(f"âŒ MQTTè¿æ¥å¤±è´¥: {e}")
            return False

    def initialize_tts(self):
        """åˆå§‹åŒ–ç¦»çº¿TTSå¼•æ“"""
        try:
            import pyttsx3
            self.tts_engine = pyttsx3.init(driverName='espeak')
            self.tts_engine.setProperty('rate', TTS_RATE)
            self.tts_engine.setProperty('volume', TTS_VOLUME)
            
            voices = self.tts_engine.getProperty('voices')
            print("å¯ç”¨è¯­éŸ³åˆ—è¡¨ï¼š")
            for i, voice in enumerate(voices):
                print(f"ç´¢å¼• {i}ï¼šID={voice.id}ï¼Œåç§°={voice.name}ï¼Œè¯­è¨€={voice.languages}")
            
            # é€‰æ‹©ä¸­æ–‡è¯­éŸ³
            selected_voice = None
            if len(voices) > 12:  # ä¼˜å…ˆé€‰æ‹©å·²çŸ¥ä¸­æ–‡è¯­éŸ³ç´¢å¼•
                selected_voice = voices[12].id
            else:  # æŒ‰è¯­è¨€æ ‡è¯†åŒ¹é…
                for voice in voices:
                    for lang in voice.languages:
                        if isinstance(lang, bytes) and lang.decode('utf-8').startswith(('cmn', 'zh')):
                            selected_voice = voice.id
                            break
            if selected_voice:
                self.tts_engine.setProperty('voice', selected_voice)
                print(f"âœ… æˆåŠŸè®¾ç½®ä¸­æ–‡è¯­éŸ³ï¼š{selected_voice}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡è¯­éŸ³ï¼Œå¯èƒ½å¯¼è‡´æ’­æŠ¥å¼‚å¸¸")
            
            return True
        except Exception as e:
            print(f"âŒ ç¦»çº¿TTSåˆå§‹åŒ–å¤±è´¥: {e}")
            self.tts_engine = None
            return False

    def speak(self, text):
        """ç¦»çº¿è¯­éŸ³æ’­æŠ¥"""
        filtered_text = re.sub(r'[^\u4e00-\u9fa5ï¼Œã€‚ï¼Ÿï¼,.;?!\s]', '', text).strip()
        if not filtered_text:
            print("âš ï¸ è¿‡æ»¤åæ— æœ‰æ•ˆæ–‡æœ¬ï¼Œæ— æ³•æ’­æŠ¥")
            return
        def _speak_offline():
            if not self.tts_engine:
                print(f"âš ï¸ TTSæœªåˆå§‹åŒ–ï¼Œæ— æ³•æ’­æŠ¥: {text}")
                return
            try:
                with self.tts_lock:  # åŸæœ‰é”ï¼šé˜²æ­¢åŒæ—¶æ’­æŠ¥
                    with self.audio_block_lock:  # æ–°å¢é”ï¼šé˜»æ–­éŸ³é¢‘é‡‡é›†
                        self.tts_engine.say(filtered_text)
                        self.tts_engine.runAndWait()
            except Exception as e:
                print(f"âŒ ç¦»çº¿æ’­æŠ¥é”™è¯¯: {e}")
        threading.Thread(target=_speak_offline, daemon=True).start()

    # ------------------------------
    # ä»»åŠ¡ç‚¹ç®¡ç†æ ¸å¿ƒåŠŸèƒ½
    # ------------------------------
    def _load_task_points(self):
        """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡ç‚¹"""
        if os.path.exists(TASK_POINTS_FILE):
            try:
                with open(TASK_POINTS_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½ä»»åŠ¡ç‚¹å¤±è´¥: {e}")
        return {}  # åˆå§‹ä¸ºç©ºå­—å…¸

    def _save_task_points(self):
        """ä¿å­˜ä»»åŠ¡ç‚¹åˆ°æ–‡ä»¶ï¼ˆå¢åŠ æ—¥å¿—å’Œå®¹é”™ï¼‰"""
        try:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(os.path.dirname(TASK_POINTS_FILE)):
                os.makedirs(os.path.dirname(TASK_POINTS_FILE), exist_ok=True)
                print(f"ğŸ“‚ åˆ›å»ºç›®å½•: {os.path.dirname(TASK_POINTS_FILE)}")
            
            # å†™å…¥æ–‡ä»¶
            with open(TASK_POINTS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.task_points, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ä»»åŠ¡ç‚¹å·²ä¿å­˜è‡³ {TASK_POINTS_FILE}ï¼ˆå†…å®¹ï¼š{self.task_points}ï¼‰")
            return True  # ä¿å­˜æˆåŠŸ
        except PermissionError:
            print(f"âŒ ä¿å­˜å¤±è´¥ï¼šæ— æƒé™å†™å…¥æ–‡ä»¶ {TASK_POINTS_FILE}")
            return False
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{e}ï¼ˆæ–‡ä»¶è·¯å¾„ï¼š{TASK_POINTS_FILE}ï¼‰")
            return False
            
    def _get_scene_description(self, frame):
        """ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„å¸§æ•°æ®"""
        try:
            if frame is None:
                return "æœªè·å–åˆ°ç”»é¢"
            
            # ç›´æ¥è½¬æ¢å¸§ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            
            # prompt = """è¯†åˆ«å›¾ç‰‡ä¸­çš„æ ¸å¿ƒç‰©å“ï¼ˆ1-2ä¸ªï¼‰ï¼Œéµå¾ªï¼š
            # 1. åªè¯´ç‰©ä½“åç§°ï¼ˆå¦‚"æ‰“å°æœº"ï¼‰ï¼›
            # 2. å¿½ç•¥é¢œè‰²ã€ä½ç½®ç­‰ä¿®é¥°ï¼›
            # 3. è‹¥æœ‰å¤šä¸ªï¼Œé€‰æœ€æ˜¾çœ¼çš„ã€‚
            # åå­—ä»¥å†…å®Œæˆã€‚
            # """
            prompt = """è¯†åˆ«å›¾ç‰‡ä¸­çš„æ ¸å¿ƒç‰©å“ï¼ˆ1-2ä¸ªï¼‰ï¼Œéµå¾ªï¼š
            1. åªè¯´ç‰©ä½“åç§°ï¼ˆå¦‚"æ‰“å°æœº"ï¼‰ï¼›
            2. å¿½ç•¥é¢œè‰²ã€ä½ç½®ç­‰ä¿®é¥°ï¼›
            3. è‹¥æœ‰å¤šä¸ªï¼Œé€‰æœ€æ˜¾çœ¼çš„ã€‚
            åå­—ä»¥å†…å®Œæˆã€‚
            # """
            messages = [
                {"role": "observer", "content": [
                    {"type": "image", "image": pil_image},  # ç›´æ¥ä½¿ç”¨PILå¯¹è±¡
                    {"type": "text", "text": prompt}
                ]}
            ]
            
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)
            inputs = self.qwen_processor(
                text=[text], images=image_inputs, padding=True, return_tensors="pt"
            ).to(self.device)
            
            generated_ids = self.qwen_model.generate(** inputs, max_new_tokens=50)
            generated_ids_trimmed = generated_ids[0][len(inputs['input_ids'][0]):]
            description = self.qwen_processor.decode(generated_ids_trimmed, skip_special_tokens=True).strip()
            print(f"description: {description}")
            return description if description else "åœºæ™¯æè¿°å¤±è´¥"
        except Exception as e:
            print(f"åœºæ™¯æè¿°é”™è¯¯: {e}")
            return "åœºæ™¯è¯†åˆ«å¤±è´¥"
            
    def add_task_point(self):
        """æ·»åŠ ä»»åŠ¡ç‚¹ï¼ˆä½¿ç”¨å½“å‰å¸§ï¼‰"""
        if not self.current_pose:
            self.speak("æœªè·å–åˆ°åº•ç›˜ä½ç½®")
            return
        
        # ä»å½“å‰ä½å§¿ä¸­æå–x, y, yaw
        x, y, yaw = self.current_pose
        
        # è·å–æœ€æ–°å¸§ï¼ˆä¿ç•™é˜Ÿåˆ—æ•°æ®ï¼‰
        latest_frame = self._get_latest_frame()
        # ä¼ é€’å¸§å‚æ•°ç»™åœºæ™¯æè¿°æ–¹æ³•
        scene_desc = self._get_scene_description(latest_frame)
        # è®¡ç®—æ–°ä»»åŠ¡ç‚¹çš„IDï¼šå–ç°æœ‰ä»»åŠ¡ç‚¹ä¸­æœ€å¤§çš„id+1ï¼Œåˆå§‹ä¸º0
        if self.task_points:
            # æå–æ‰€æœ‰ç°æœ‰ä»»åŠ¡ç‚¹çš„idå¹¶è½¬ä¸ºæ•´æ•°ï¼Œå–æœ€å¤§å€¼
            max_existing_id = max(int(point_info["id"]) for point_info in self.task_points.values())
            new_id = max_existing_id + 1
        else:
            # è‹¥æ²¡æœ‰ä»»åŠ¡ç‚¹ï¼Œåˆå§‹IDä¸º0
            new_id = 0
        
        # æ„å»ºä»»åŠ¡ç‚¹å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…å«è‡ªå¢IDï¼‰
        task_point_info = {
            "id": new_id,  # æ˜¾å¼å­˜å‚¨è‡ªå¢ID
            # ä½å§¿æ•°æ®ï¼ˆæ¥è‡ªMQTTçš„base_status.poseï¼‰
            "pose": {
                "x": round(x, 3),
                "y": round(y, 3),
                "yaw": round(yaw, 3)
            },
            # ç¯å¢ƒä¿¡æ¯ï¼ˆæ¨¡å‹è¯†åˆ«çš„åœºæ™¯æè¿°ï¼‰
            "environment": scene_desc
        }
        
        # ä»¥new_idä¸ºé”®ï¼Œä¿å­˜åˆ°ä»»åŠ¡ç‚¹å­—å…¸ï¼ˆé”®ä¸æ•°æ®ä¸­çš„idä¸€è‡´ï¼‰
        self.task_points[str(new_id)] = task_point_info
        
        # å†™å…¥JSONæ–‡ä»¶ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
        self._save_task_points()
        
        # è¯­éŸ³æ’­æŠ¥ç¡®è®¤
        self.speak(
            f"å·²æ·»åŠ ä»»åŠ¡ç‚¹{new_id}ï¼Œ"
            f"ä½ç½®ï¼šx={x:.2f}, y={y:.2f}ï¼Œ"
            f"åœºæ™¯ï¼š{scene_desc}"
        )
        print(f"æ·»åŠ ä»»åŠ¡ç‚¹{new_id}: {task_point_info}")


    def modify_task_point(self, point_id):
        """ä¿®æ”¹ä»»åŠ¡ç‚¹ï¼ˆå¢åŠ è¯¦ç»†æ—¥å¿—ï¼‰"""
        print(f"æ”¶åˆ°æŒ‡ä»¤ï¼špoint_id={point_id}") 
        # æ£€æŸ¥å½“å‰ä½å§¿
        if not self.current_pose:
            self.speak("æœªè·å–åˆ°åº•ç›˜ä½ç½®ï¼Œæ— æ³•ä¿®æ”¹ä»»åŠ¡ç‚¹")
            print("âŒ modify_task_point: current_poseä¸ºç©º")
            return
        
        # æ£€æŸ¥ä»»åŠ¡ç‚¹æ˜¯å¦å­˜åœ¨
        target_key = str(point_id)
        if target_key not in self.task_points:
            self.speak(f"æœªæ‰¾åˆ°{point_id}å·ä»»åŠ¡ç‚¹")
            print(f"âŒ modify_task_point: ä»»åŠ¡ç‚¹{point_id}ä¸å­˜åœ¨ï¼ˆå½“å‰ä»»åŠ¡ç‚¹ï¼š{list(self.task_points.keys())}ï¼‰")
            return
        
        # æ‰“å°ä¿®æ”¹å‰çš„æ•°æ®
        print(f"ä¿®æ”¹å‰ - ä»»åŠ¡ç‚¹{point_id}: {self.task_points[target_key]}")
        
        # è·å–æ–°æ•°æ®
        x, y, yaw = self.current_pose
        # è·å–æœ€æ–°å¸§ï¼ˆä¿ç•™é˜Ÿåˆ—æ•°æ®ï¼‰
        latest_frame = self._get_latest_frame()
        # ä¼ é€’å¸§å‚æ•°ç»™åœºæ™¯æè¿°æ–¹æ³•
        scene_desc = self._get_scene_description(latest_frame)     
        print(f"æ–°æ•°æ® - ä½å§¿: (x={x}, y={y}, yaw={yaw}), åœºæ™¯: {scene_desc}")
        
        # æ›´æ–°å†…å­˜ä¸­çš„ä»»åŠ¡ç‚¹
        self.task_points[target_key] = {
            "id": int(point_id),
            "pose": {
                "x": round(x, 3),
                "y": round(y, 3),
                "yaw": round(yaw, 3)
            },
            "environment": scene_desc
        }
        
        # æ‰“å°ä¿®æ”¹åçš„æ•°æ®
        print(f"ä¿®æ”¹å - ä»»åŠ¡ç‚¹{point_id}: {self.task_points[target_key]}")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_success = self._save_task_points()  # ä¿®æ”¹_save_task_pointsä½¿å…¶è¿”å›æ˜¯å¦æˆåŠŸ
        if save_success:
            self.speak(f"å·²å°†å½“å‰ä½ç½®è®¾ä¸º{point_id}å·ç‚¹ï¼Œåœºæ™¯ï¼š{scene_desc}")
        else:
            self.speak(f"ä¿®æ”¹ä»»åŠ¡ç‚¹{point_id}å¤±è´¥ï¼Œæ–‡ä»¶ä¿å­˜å‡ºé”™")

    
    
    # ------------------------------
    # MQTTç›¸å…³å›è°ƒ
    # ------------------------------
    def on_connect(self, client, userdata, flags, rc, properties=None):
        print(f"MQTTè¿æ¥æˆåŠŸ,çŠ¶æ€ç : {rc}")
        client.subscribe(self.mqtt_status_topic)  # è®¢é˜…åº•ç›˜çŠ¶æ€è¯é¢˜
        client.subscribe(self.mqtt_control_topic)  # è®¢é˜…æ§åˆ¶æŒ‡ä»¤è¯é¢˜
        client.subscribe("script/grab_status")   # è®¢é˜…æœºæ¢°è‡‚æŠ“å–çŠ¶æ€è¯é¢˜

    def on_message(self, client, userdata, msg):
        try:
            payload = json.loads(msg.payload.decode())
            
            # ä»…å¤„ç†æ¥è‡ª "script/grab_status" è¯é¢˜çš„æœºæ¢°è‡‚çŠ¶æ€ï¼ˆå¾ªç¯å¯¼èˆªä¸­ï¼‰
            if msg.topic == "script/grab_status" and "grab_successfully" in payload and self.is_cycling:
                arm_status = payload.get('grab_successfully', -1)
                current_point = self.cycle_order[self.current_cycle_step]  # å½“å‰ä»»åŠ¡ç‚¹ï¼ˆä»å¾ªç¯é¡ºåºå–ï¼‰
                print(f"ä» {msg.topic} æ”¶åˆ°æœºæ¢°è‡‚çŠ¶æ€: {arm_status}ï¼ˆå½“å‰ç‚¹ï¼š{current_point}ï¼Œå¾ªç¯æ­¥éª¤ï¼š{self.current_cycle_step}ï¼‰")

                # çŠ¶æ€ç æ˜ å°„ï¼šæœºæ¢°è‡‚çŠ¶æ€â†’ç›®æ ‡ç‚¹IDï¼ˆç›´æ¥æŒ‡å®šä¸‹ä¸€æ­¥è¦å»çš„ç‚¹ï¼‰
                status_to_target = {
                    1: 2,  # çŠ¶æ€1â†’å»2å·ç‚¹
                    2: 3,  # çŠ¶æ€2â†’å»3å·ç‚¹
                    3: 0,  # çŠ¶æ€3â†’å»0å·ç‚¹
                    0: None,  # çŠ¶æ€0â†’å¾ªç¯ç»“æŸ
                    -1: None  # æ— æ•ˆçŠ¶æ€â†’ä¸æ“ä½œ
                }

                # è·å–ç›®æ ‡ç‚¹
                target_point = status_to_target.get(arm_status, None)

                # å¤„ç†ä¸åŒçŠ¶æ€åœºæ™¯
                if arm_status == -1:
                    self.speak("æœªè·å–åˆ°æœºæ¢°è‡‚æœ‰æ•ˆå·¥ä½œçŠ¶æ€ï¼Œå¾ªç¯æš‚åœ")
                    self.is_cycling = False  # æš‚åœå¾ªç¯
                    return

                elif arm_status == 0:
                    # çŠ¶æ€0ï¼š0å·ç‚¹å®Œæˆï¼ˆå¾ªç¯ç»“æŸï¼‰
                    self.completed_cycles += 1
                    self.speak(f"å·²å®Œæˆå•æ¬¡å¾ªç¯ä»»åŠ¡ï¼ˆ1â†’2â†’3â†’0ï¼‰ï¼Œå…±å®Œæˆ{self.completed_cycles}æ¬¡")
                    self.is_cycling = False  # ç»ˆæ­¢å¾ªç¯
                    return

                elif target_point is not None:
                    # è®¡ç®—ä¸‹ä¸€æ­¥éª¤ç´¢å¼•ï¼ˆæ ¹æ®ç›®æ ‡ç‚¹åœ¨å¾ªç¯é¡ºåºä¸­çš„ä½ç½®ï¼‰
                    try:
                        next_step = self.cycle_order.index(target_point)
                        self.speak(f"æœºæ¢°è‡‚åœ¨{current_point}å·ç‚¹å·¥ä½œå®Œæˆï¼Œå³å°†å‰å¾€{target_point}å·ç‚¹ï¼ˆå¾ªç¯æ­¥éª¤{next_step+1}/4ï¼‰")
                        
                        # æ›´æ–°å¾ªç¯æ­¥éª¤è®¡æ•°å™¨
                        self.current_cycle_step = next_step
                        # å¯¼èˆªåˆ°ç›®æ ‡ç‚¹
                        if str(target_point) in self.task_points:
                            self.send_goal_point(str(target_point))
                        else:
                            self.speak(f"å¾ªç¯ä¸­æ–­ï¼šæœªæ‰¾åˆ°{target_point}å·ä»»åŠ¡ç‚¹")
                            self.is_cycling = False
                    except ValueError:
                        self.speak(f"å¾ªç¯é…ç½®é”™è¯¯ï¼šç›®æ ‡ç‚¹{target_point}ä¸åœ¨å¾ªç¯é¡ºåºä¸­")
                        self.is_cycling = False

                else:
                    # æœªçŸ¥çŠ¶æ€
                    self.speak(f"æ”¶åˆ°æœªçŸ¥æœºæ¢°è‡‚çŠ¶æ€{arm_status}ï¼Œå¾ªç¯æš‚åœ")
                    self.is_cycling = False
                return
            
            # å¤„ç†åº•ç›˜çŠ¶æ€ï¼ˆæ›´æ–°å½“å‰ä½å§¿ï¼‰
            if msg.topic == self.mqtt_status_topic and "pose" in payload:
                pose = payload["pose"]
                self.current_pose = (pose["x"], pose["y"], pose["yaw"])
                # print(f"æ›´æ–°å½“å‰ä½å§¿: x={pose['x']:.2f}, y={pose['y']:.2f}, yaw={pose['yaw']:.2f}")
                    
        except json.JSONDecodeError:
            print("MQTTæ¶ˆæ¯è§£æå¤±è´¥")
            self.speak("æŒ‡ä»¤è§£æé”™è¯¯ï¼Œè¯·é‡è¯•")
        except Exception as e:
            print(f"å¤„ç†MQTTæ¶ˆæ¯é”™è¯¯: {e}")

        
    # ------------------------------
    # å‘½ä»¤å¤„ç†
    # ------------------------------
    def process_image_recognition(self):
        """ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„å¸§æ•°æ®"""
        try:
            self.speak("æ­£åœ¨è¯†åˆ«ï¼Œè¯·ç¨å€™...")
            print("å¼€å§‹å›¾åƒè¯†åˆ«...")
            # è·å–æœ€æ–°å¸§ï¼ˆä¸æ¸…ç©ºé˜Ÿåˆ—ï¼‰
            latest_frame = self._get_latest_frame()

            if latest_frame is None:
                error_msg = "æœªè·å–åˆ°æ‘„åƒå¤´ç”»é¢"
                self.speak(error_msg)
                print(error_msg)
                return
                
            # ç›´æ¥è½¬æ¢å¸§ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(cv2.cvtColor(latest_frame, cv2.COLOR_BGR2RGB))
            # åœ¨è½¬æ¢ä¸ºPILå›¾åƒåæ·»åŠ resize
            target_size = (768, 768)  # æ¨¡å‹æ¨èå°ºå¯¸
            pil_image = pil_image.resize(target_size, Image.LANCZOS)  # é«˜è´¨é‡ç¼©æ”¾

            prompt = """è¯†åˆ«å›¾ç‰‡ä¸­çš„æ ¸å¿ƒç‰©å“ï¼ˆ1-2ä¸ªï¼‰ï¼Œéµå¾ªï¼š
            1. åªè¯´ç‰©ä½“åç§°ï¼›
            2. å¿½ç•¥é¢œè‰²ã€ä½ç½®ç­‰ä¿®é¥°ï¼›
            3. è‹¥æœ‰å¤šä¸ªï¼Œé€‰æœ€æ˜¾çœ¼çš„ã€‚
            åå­—ä»¥å†…å®Œæˆã€‚
            """
            messages = [
                {"role": "observer", "content": [
                    {"type": "image", "image": pil_image},  # ç›´æ¥ä½¿ç”¨PILå¯¹è±¡
                    {"type": "text", "text": prompt}
                ]}
            ]
            
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)
            inputs = self.qwen_processor(
                text=[text], images=image_inputs, padding=True, return_tensors="pt"
            ).to(self.device)
            
            generated_ids = self.qwen_model.generate(** inputs, max_new_tokens=100)
            generated_ids_trimmed = generated_ids[0][len(inputs['input_ids'][0]):]
            result = self.qwen_processor.decode(generated_ids_trimmed, skip_special_tokens=True).strip()
            
            print(f"å›¾åƒè¯†åˆ«ç»“æœ: {result}")
            # æå–æ•°å­—å¹¶åŒ¹é…ä»»åŠ¡ç‚¹
            self.match_task_point_by_number(result)

            self.speak(f"è¯†åˆ«åˆ°ï¼š{result}")
            self.speak("è¿˜æœ‰å…¶ä»–æŒ‡ä»¤å—ï¼Ÿ")
            
        except Exception as e:
            print(f"å›¾åƒè¯†åˆ«é”™è¯¯: {e}")
            self.speak("è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•")
        finally:
            torch.cuda.empty_cache()
    def extract_numbers_from_result(self, recognition_result: str) -> list:
        """å¢å¼ºç‰ˆï¼šæ”¯æŒå¤åˆæ•°å­—ã€å­—æ¯ç»„åˆåŠåŠ¨æ€æ˜ å°„"""
        if not isinstance(recognition_result, str):
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹")

        # åŠ è½½åŠ¨æ€æ˜ å°„è¡¨ï¼ˆç¤ºä¾‹ï¼‰
        self.char_to_id = {
            # ä¸­æ–‡æ•°å­—ï¼ˆ0-9ï¼‰
            "é›¶": 0, "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5, "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9,
            # å­—æ¯ï¼ˆA-D/a-d â†’ 0-3ï¼‰
            "A": 0, "B": 1, "C": 2, "D": 3,
            "a": 0, "b": 1, "c": 2, "d": 3,
            "0": 0, "1": 1, "2": 2, "3": 3
        }
        task_ids = []

        # -------------------- æå–é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆæ”¯æŒå¤åˆæ•°å­—ï¼‰ --------------------
        # åŒ¹é…è¿ç»­æ•°å­—ï¼ˆå¦‚"123"æˆ–"1å·"ä¸­çš„"1"ï¼‰
        arabic_pattern = r"\d+"
        arabic_matches = re.findall(arabic_pattern, recognition_result)
        for match in arabic_matches:
            task_ids.extend([int(d) for d in match])  # æ”¯æŒå¤šä½æ•°å­—æ‹†åˆ†ï¼ˆå¦‚"123"â†’[1,2,3]ï¼‰

        # -------------------- æå–ä¸­æ–‡æ•°å­—ï¼ˆæ”¯æŒå¤åˆè¯ï¼‰ --------------------
        # åŒ¹é…ä¸­æ–‡æ•°å­—ï¼ˆå¦‚"ä¸‰å·"â†’3ï¼‰
        chinese_num_pattern = r"(é›¶|ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹)+(å·|ç‚¹|å±‚)?"
        chinese_matches = re.findall(chinese_num_pattern, recognition_result)
        for match in chinese_matches:
            num_str = match[0]
            if num_str in self.char_to_id:
                task_ids.append(self.char_to_id[num_str])

        # -------------------- æå–å­—æ¯ï¼ˆæ”¯æŒå¤§å°å†™æ··åˆï¼‰ --------------------
        # åŒ¹é…å­—æ¯ç»„åˆï¼ˆå¦‚"A3"â†’13ï¼‰
        letter_pattern = r"[A-Za-z0-9]+"
        letter_matches = re.findall(letter_pattern, recognition_result)
        for match in letter_matches:
            key = match[0].upper() if match else ""
            task_ids.append(self.char_to_id.get(key, -1))

        # -------------------- è¿‡æ»¤æ— æ•ˆIDå¹¶å»é‡ --------------------
        valid_ids = [tid for tid in set(task_ids) if str(tid) in self.task_points.keys()]
        return valid_ids
    
    def match_task_point_by_number(self, recognition_result: str):
        """æ·»åŠ è°ƒè¯•ä¿¡æ¯"""
        if not isinstance(recognition_result, str):
            self.speak("è¯†åˆ«ç»“æœæ ¼å¼å¼‚å¸¸")
            return

        task_ids = self.extract_numbers_from_result(recognition_result)
        print(f"ã€è°ƒè¯•ã€‘æå–çš„å€™é€‰ä»»åŠ¡ç‚¹ID: {task_ids}")  # æ–°å¢æ—¥å¿—

        valid_points = []
        for tid in task_ids:
            point_key = str(tid)
            if point_key in self.task_points:
                valid_points.append((tid, self.task_points[point_key]))
            else:
                print(f"ã€è­¦å‘Šã€‘ä»»åŠ¡ç‚¹ID {tid} æœªåœ¨é…ç½®ä¸­å®šä¹‰")  # æ–°å¢æ—¥å¿—

        if not valid_points:
            id_str = "ã€".join(map(str, task_ids))
            self.speak(f"æœªæ‰¾åˆ°IDä¸º {id_str} çš„ä»»åŠ¡ç‚¹ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return

        target_id, target_info = valid_points[0]
        print(f"ã€è°ƒè¯•ã€‘åŒ¹é…åˆ°ä»»åŠ¡ç‚¹: {target_id} â†’ {target_info}")  # æ–°å¢æ—¥å¿—
        self.send_goal_point(str(target_id)) # å‘é€åŒ¹é…åˆ°çš„ä»»åŠ¡ç‚¹

    def process_dialogue(self, text):
        """å¤„ç†å¯¹è¯è¯·æ±‚"""
        try:
            self.speak("æ­£åœ¨æ€è€ƒï¼Œè¯·ç¨å€™...")
            print(f"å¤„ç†å¯¹è¯: {text}")
            
            messages = [{"role": "friend", "content": [{"type": "text", "text": text}]}]
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.qwen_processor(text=[text], padding=True, return_tensors="pt").to(self.device)
            
            generated_ids = self.qwen_model.generate(**inputs, max_new_tokens=200)
            generated_ids_trimmed = generated_ids[0][len(inputs['input_ids'][0]):]
            response = self.qwen_processor.decode(generated_ids_trimmed, skip_special_tokens=True).strip()
            
            print(f"å¯¹è¯å›å¤: {response}")
            #å¯¹åº”josnç¯å¢ƒæè¿°

            # æ£€æŸ¥å¯¹è¯å†…å®¹æ˜¯å¦æ¶‰åŠä»»åŠ¡ç‚¹ç¯å¢ƒæè¿°ï¼Œè‹¥æ¶‰åŠåˆ™å¯¼èˆª
            self.check_dialogue_for_environment(text, response)

            self.speak(response)
        except Exception as e:
            print(f"å¯¹è¯å¤„ç†é”™è¯¯: {e}")
            self.speak("å¤„ç†å¯¹è¯æ—¶å‡ºé”™ï¼Œè¯·é‡è¯•")

    def check_dialogue_for_environment(self, user_query, model_response):
        """
        ä½¿ç”¨åƒé—®æ¨¡å‹åˆ†æå¯¹è¯å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦æ¶‰åŠä»»åŠ¡ç‚¹ç¯å¢ƒæè¿°
        è‹¥æ¶‰åŠåˆ™æŸ¥æ‰¾å¯¹åº”ä»»åŠ¡ç‚¹å¹¶å‘é€å¯¼èˆªæŒ‡ä»¤
        """
        if not self.task_points:
            print("æ— ä»»åŠ¡ç‚¹æ•°æ®ï¼Œè·³è¿‡ç¯å¢ƒåŒ¹é…")
            return
            
        # å‡†å¤‡å½“å‰æ‰€æœ‰ä»»åŠ¡ç‚¹çš„ç¯å¢ƒæè¿°ï¼Œä¾›æ¨¡å‹å‚è€ƒ
        environment_list = [f"ID {k}: {v['environment']}" for k, v in self.task_points.items()]
        environments_text = "\n".join(environment_list)
        
        # æ„å»ºæç¤ºè¯ï¼Œè®©æ¨¡å‹åˆ†æå¯¹è¯æ˜¯å¦æ¶‰åŠä»»åŠ¡ç‚¹ç¯å¢ƒ
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦æ¶‰åŠéœ€è¦å¯¼èˆªçš„ç¯å¢ƒåœ°ç‚¹ï¼š
        
        ç”¨æˆ·é—®ï¼š{user_query}
        å›å¤ï¼š{model_response}
        
        ç°æœ‰å¯å¯¼èˆªçš„ç¯å¢ƒåœ°ç‚¹åˆ—è¡¨ï¼š
        {environments_text}
        
        è¯·æŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†ï¼š
        1. å¦‚æœå¯¹è¯ä¸­æ˜ç¡®æåˆ°äº†åˆ—è¡¨ä¸­çš„æŸä¸ªç¯å¢ƒåœ°ç‚¹ï¼Œè¯·è¿”å›è¯¥åœ°ç‚¹çš„ID
        2. å¦‚æœæåˆ°çš„å†…å®¹ä¸æŸä¸ªç¯å¢ƒåœ°ç‚¹é«˜åº¦ç›¸å…³ï¼Œè¯·è¿”å›è¯¥åœ°ç‚¹çš„ID
        3. å¦‚æœæœªæåˆ°ä»»ä½•ç¯å¢ƒåœ°ç‚¹æˆ–æ— æ³•ç¡®å®šï¼Œè¯·è¿”å›-1
        4. åªè¿”å›æ•°å­—IDï¼Œä¸è¦è¿”å›ä»»ä½•é¢å¤–æ–‡å­—
        5. æ ¹æ®{user_query}{model_response}åˆ¤æ–­æ˜¯å¦æœ‰ç¬¦åˆå¯¹è¯å†…å®¹è¦æ±‚çš„{environments_text}ï¼Œè¿”å›å…¶æ•°å­—ID
        
        # ä¾‹å¦‚ï¼š
        # ç”¨æˆ·é—®ï¼š"æ‰“å°æœºåœ¨å“ªé‡Œï¼Ÿ"
        # å›å¤ï¼š"æ‰“å°æœºåœ¨3å·ç‚¹"
        # åˆ—è¡¨ä¸­æœ‰"ID 3: æ‰“å°æœº"
        # åˆ™è¿”å›ï¼š3
        """
        
        try:
            # è°ƒç”¨åƒé—®æ¨¡å‹è¿›è¡Œåˆ†æ
            messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = self.qwen_processor(text=[text], padding=True, return_tensors="pt").to(self.device)
            
            generated_ids = self.qwen_model.generate(
                **inputs, 
                max_new_tokens=10,  # åªéœ€è¦è¿”å›ä¸€ä¸ªæ•°å­—ï¼Œé™åˆ¶è¾“å‡ºé•¿åº¦
                temperature=0.1,    # é™ä½éšæœºæ€§ï¼Œç¡®ä¿ç»“æœç¨³å®š
                top_p=0.9
            )
            
            generated_ids_trimmed = generated_ids[0][len(inputs['input_ids'][0]):]
            result = self.qwen_processor.decode(generated_ids_trimmed, skip_special_tokens=True).strip()
            
            # è§£ææ¨¡å‹è¿”å›çš„ID
            if result.isdigit():
                target_id = int(result)
                target_key = str(target_id)
                
                if target_key in self.task_points:
                    print(f"å¯¹è¯ä¸­è¯†åˆ«åˆ°ç¯å¢ƒåœ°ç‚¹ï¼šID {target_id}ï¼ˆ{self.task_points[target_key]['environment']}ï¼‰")
                    self.speak(f"æ£€æµ‹åˆ°æ‚¨æåˆ°äº†{self.task_points[target_key]['environment']}ï¼Œæ˜¯å¦éœ€è¦å¯¼èˆªè¿‡å»ï¼Ÿ")
                    
                    # ç›´æ¥å‘é€æ£€æµ‹åˆ°çš„ç‚¹ï¼Ÿ or é€šè¿‡ä»€ä¹ˆæ–¹å¼ç¡®è®¤å»ç‚¹ï¼Ÿ
                    # ç›´æ¥å‘é€ç‚¹
                    self.send_goal_point(target_key)
                else:
                    print(f"æ¨¡å‹è¿”å›çš„ID {target_id} ä¸åœ¨ä»»åŠ¡ç‚¹åˆ—è¡¨ä¸­")
            else:
                print(f"å¯¹è¯ä¸­æœªè¯†åˆ«åˆ°éœ€è¦å¯¼èˆªçš„ç¯å¢ƒåœ°ç‚¹ï¼ˆæ¨¡å‹è¿”å›ï¼š{result}ï¼‰")
                
        except Exception as e:
            print(f"åˆ†æå¯¹è¯ç¯å¢ƒæè¿°æ—¶å‡ºé”™ï¼š{e}")

    def send_lerobot(self, number):
        """å‘lerobot_statusè¯é¢˜å‘é€æŒ‡å®šçš„é˜¿æ‹‰ä¼¯æ•°å­—"""
        # æ ¹æ®ä¼ å…¥çš„æ•°å­—ï¼Œæ‰§è¡Œå¯¹åº”æœºæ¢°è‡‚åŠ¨ä½œå¦‚ä¸‹ 0ï½3ï¼šé¡ºåºå¯¹åº”èµ·å§‹ç‚¹ ä¸€å·ç‚¹ äºŒå·ç‚¹ ä¸‰å¥½ç‚¹  9ï¼šæ‰“æ‹›å‘¼  6ï¼šå‘ŠçŸ¥å¼€å§‹å¾ªç¯ä»»åŠ¡ï¼Œæœºæ¢°è‡‚å·¥ä½œå®Œæˆç»™åé¦ˆ
        try:
            # ç¡®ä¿ä¼ å…¥çš„æ˜¯æœ‰æ•ˆçš„é˜¿æ‹‰ä¼¯æ•°å­—
            if not isinstance(number, int) or number < 0:
                raise ValueError(f"æ— æ•ˆçš„æ•°å­—: {number}ï¼Œå¿…é¡»æ˜¯éè´Ÿæ•´æ•°")
            
            # æ„é€ æ¶ˆæ¯
            message = {
                "type": "greeting",
                "number": number
            }
            command_json = json.dumps(message)
            
            # å‘å¸ƒåˆ°lerobot_statusè¯é¢˜ï¼ˆè¿ç»­å‘3æ¬¡ç¡®ä¿é€è¾¾ï¼‰
            for _ in range(3):
                self.mqtt_client.publish(
                    topic="lerobot_status",  # ç›®æ ‡è¯é¢˜
                    payload=command_json,
                    qos=2  # ç¡®ä¿æ¶ˆæ¯å¯é ä¼ é€’
                )
                time.sleep(0.1)
            
            print(f"âœ… å·²å‘lerobot_statuså‘é€æ•°å­—: {number}") # å‘é€ä¿¡æ¯ï¼Œæœºæ¢°è‡‚æ‰§è¡ŒåŠ¨ä½œ
            
            if number == 9:  # ä»…æ‰“æ‹›å‘¼ä»»åŠ¡æ’­æŠ¥
                self.speak(f"ä½ å¥½ï¼Œæˆ‘æ˜¯å°è½¦ï¼Œéœ€è¦å¸®åŠ©å—")  # æŒ‡ä»¤é—®å€™æ’­æŠ¥é—®å€™è¯­
        except Exception as e:
            print(f"âŒ å‘é€æ‰“æ‹›å‘¼æ¶ˆæ¯å¤±è´¥: {e}")
            self.speak("æ‰“æ‹›å‘¼å¤±è´¥ï¼Œè¯·é‡è¯•")

    # ------------------------------
    # å¯¼èˆªåŠŸèƒ½ï¼ˆåˆå¹¶æŒ‰æè¿°å¯¼èˆªï¼‰
    # ------------------------------
    def handle_navigation(self, command_code=None, query_text=""):
        """
        å¤„ç†æ‰€æœ‰å¯¼èˆªç›¸å…³æŒ‡ä»¤ï¼ŒåŒ…æ‹¬ï¼š
        1. æŒ‰ä»»åŠ¡ç‚¹IDå¯¼èˆªï¼ˆå¦‚å»1å·ç‚¹ï¼‰
        2. æŒ‰æè¿°å¯¼èˆªï¼ˆå¦‚å»æ‹¿æ°´ï¼‰
        """
        if command_code is not None and int(command_code) == 6:
            self.start_cycle_navigation()  # å¯åŠ¨å¾ªç¯å¯¼èˆª
            return True
        if command_code is not None and int(command_code) == 7:
            self.is_cycling = False
            self.speak("å·²åœæ­¢å¾ªç¯ä»»åŠ¡")
            return True
        
        # å°è¯•æŒ‰ç¯å¢ƒæè¿°åŒ¹é…ä»»åŠ¡ç‚¹
        matched_point = None
        if query_text:
            print(f"å°è¯•æŒ‰æè¿°å¯¼èˆª: {query_text}")
            matched_point = self.find_point_by_environment(query_text)
        
        # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„ä»»åŠ¡ç‚¹ï¼Œå¯¼èˆªåˆ°è¯¥ç‚¹
        if matched_point:
            self.speak(f"æ‰¾åˆ°åŒ¹é…çš„ä½ç½®ï¼Œæ­£åœ¨å¯¼èˆªè¿‡å»")
            success = self.send_goal_point(matched_point)
            if success:
                return True
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨æŒ‡å®šçš„ä»»åŠ¡ç‚¹IDå¯¼èˆª
        if command_code is not None:
            print(f"æŒ‰ä»»åŠ¡ç‚¹IDå¯¼èˆª: {command_code}")

            # ç¡®ä¿ä»»åŠ¡ç‚¹å­˜åœ¨
            target_key = str(command_code)
            if target_key in self.task_points:
                self.send_goal_point(target_key)
                return True
            else:
                self.speak(f"æœªæ‰¾åˆ°{command_code}å·ä»»åŠ¡ç‚¹ï¼Œè¯·å…ˆæ·»åŠ ")
                return False

        # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»åŠ¡ç‚¹IDä¸”æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œæç¤ºç”¨æˆ·
        self.speak("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å¯¼èˆªç›®æ ‡ï¼Œè¯·æä¾›æ›´æ˜ç¡®çš„æŒ‡ä»¤")
        return False

    # ------------------------------
    # ç³»ç»Ÿåˆå§‹åŒ–ä¸è¿è¡Œ
    # ------------------------------
    def _get_camera_capture(self):
        """æ‰“å¼€æ‘„åƒå¤´ï¼ˆä¼˜å…ˆé€‰æ‹©å¤–æ¥USBæ‘„åƒå¤´ï¼‰"""
        import glob
        import re
        camera_devices = glob.glob("/dev/video*")
        if not camera_devices:
            print("âŒ æœªæ£€æµ‹åˆ°æ‘„åƒå¤´è®¾å¤‡")
            return None
        
        # æ’åºè®¾å¤‡ï¼šä¼˜å…ˆé€‰æ‹©ç¼–å·è¾ƒé«˜çš„è®¾å¤‡ï¼ˆé€šå¸¸å¤–æ¥USBæ‘„åƒå¤´ç¼–å·å¤§äºå†…ç½®ï¼‰
        camera_devices.sort(key=lambda x: int(re.findall(r'\d+', x)[0]), reverse=True)
        
        for dev_path in camera_devices:
            try:
                # å°è¯•æ‰“å¼€è®¾å¤‡ï¼Œè®¾ç½®å‚æ•°ï¼ˆé€‚é…USBæ‘„åƒå¤´å¸¸è§åˆ†è¾¨ç‡ï¼‰
                cap = cv2.VideoCapture(dev_path)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
                cap.set(cv2.CAP_PROP_FPS, 30)
                
                if cap.isOpened():
                    # éªŒè¯æ˜¯å¦èƒ½è·å–å¸§
                    ret, _ = cap.read()
                    if ret:
                        print(f"âœ… æˆåŠŸæ‰“å¼€å¤–æ¥æ‘„åƒå¤´: {dev_path}")
                        return cap
                    else:
                        cap.release()
            except Exception as e:
                print(f"å°è¯•æ‰“å¼€{dev_path}å¤±è´¥: {e}")
        
        print("âŒ æ‰€æœ‰æ‘„åƒå¤´è®¾å¤‡å‡æ— æ³•æ‰“å¼€")
        return None
    
    def _get_latest_frame(self):
        """è·å–æœ€æ–°è§†é¢‘å¸§ï¼ˆä¸æ¸…ç©ºé˜Ÿåˆ—ï¼‰"""
        latest_frame = None
        temp_frames = []
        # å–å‡ºæ‰€æœ‰å¸§å¹¶ç¼“å­˜
        while not self.video_queue.empty():
            temp_frames.append(self.video_queue.get())
        
        # è·å–æœ€åä¸€å¸§
        if temp_frames:
            latest_frame = temp_frames[-1][0]
            # å°†æ‰€æœ‰å¸§æ”¾å›é˜Ÿåˆ—ï¼ˆä¿ç•™æ•°æ®ï¼‰
            for frame, ts in temp_frames:
                self.video_queue.put((frame, ts))
        
        return latest_frame
    
    def initialize_system(self):
        try:
            if not self.initialize_tts():
                return False
            if not self.initialize_models():
                return False
            self.pyaudio_instance = pyaudio.PyAudio()
            self.audio_stream = self.pyaudio_instance.open(
                format=self.FORMAT,
                channels=AUDIO_CHANNELS,
                rate=AUDIO_RATE,
                input=True,
                frames_per_buffer=AUDIO_CHUNK
            )
            self.is_running = True
            return True
        except Exception as e:
            print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def start_listening(self):
        if not self.is_running:
            return

        self.speak(self.welcome_text)
        time.sleep(2)
        self.is_listening = True
        
        self.speak(self.listening_text)
        print("\nç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œè¯·å¼€å§‹è¯´è¯...")

        threading.Thread(target=self.audio_recorder, daemon=True).start()
        threading.Thread(target=self.video_recorder, daemon=True).start()
        threading.Thread(target=self._process_commands, daemon=True).start()

    def audio_recorder(self):
        """å†…å­˜éŸ³é¢‘å¤„ç†ï¼ˆæ— æ–‡ä»¶ä¿å­˜ï¼‰"""
        self.last_active_time = time.time()
        audio_buffer = []

        while self.is_listening and self.is_running:
            # æ£€æŸ¥æ˜¯å¦æ­£åœ¨æ’­æŠ¥ï¼ˆé”è¢«å ç”¨ï¼‰ï¼Œè‹¥æ˜¯åˆ™è·³è¿‡é‡‡é›†
            if self.audio_block_lock.locked():
                time.sleep(0.01)  # çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰
                continue

            # æ­£å¸¸é‡‡é›†éŸ³é¢‘æ•°æ®
            data = self.audio_stream.read(AUDIO_CHUNK, exception_on_overflow=False)
            audio_buffer.append(data)

            if len(audio_buffer) * AUDIO_CHUNK / AUDIO_RATE >= 0.5:
                raw_audio = b''.join(audio_buffer)
                if self.check_vad_activity(raw_audio):
                    print("æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨")
                    self.last_active_time = time.time()
                    self.segments_to_save.append((raw_audio, time.time()))
                else:
                    print("é™éŸ³ä¸­...")
                audio_buffer = []

            if time.time() - self.last_active_time > NO_SPEECH_THRESHOLD:
                if self.segments_to_save and self.segments_to_save[-1][1] > self.last_vad_end_time:
                    self.process_audio_in_memory()  # æ”¹ä¸ºå†…å­˜å¤„ç†
                    self.last_active_time = time.time()

    def process_audio_in_memory(self):
        """ç›´æ¥åœ¨å†…å­˜ä¸­å¤„ç†éŸ³é¢‘"""
        if not self.segments_to_save:
            return
            
        if pygame.mixer.music.get_busy():
            pygame.mixer.music.stop()
            
        # è·å–å…³è”çš„è§†é¢‘å¸§
        start_time = self.segments_to_save[0][1]
        end_time = self.segments_to_save[-1][1]
        video_frames = []
        while not self.video_queue.empty():
            frame, timestamp = self.video_queue.get()
            if start_time <= timestamp <= end_time:
                video_frames.append(frame)
        
        # åˆå¹¶éŸ³é¢‘æ•°æ®
        audio_data = b''.join([seg[0] for seg in self.segments_to_save])
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        
        # é™é‡‡æ ·åˆ°16kHzï¼ˆè¯­éŸ³è¯†åˆ«æ¨¡å‹è¦æ±‚ï¼‰
        audio_16k = resample_poly(audio_np, 16000, AUDIO_RATE)
        audio_float32 = audio_16k.astype(np.float32) / 32768.0
        
        # ä¿å­˜åŸå§‹è¯­éŸ³æ–‡æœ¬ç”¨äºå¯¼èˆªåŒ¹é…
        self.last_nav_query = ""
        try:
            # å…ˆè¿›è¡Œè¯­éŸ³è¯†åˆ«ä»¥è·å–æ–‡æœ¬
            result = self.sensevoice_pipeline(audio_data, audio_fs=16000)
            raw_text = result[0].get("text", "").strip() if isinstance(result, list) else result.get("text", "")
            self.last_nav_query = re.sub(r'<\|.*?\|>|[^\u4e00-\u9fa5ï¼Œã€‚ï¼Ÿï¼\s]', '', raw_text)
        except:
            pass
        
        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        threading.Thread(target=self.inference, args=(video_frames, audio_float32)).start()
        
        self.saved_intervals.append((start_time, end_time))
        self.segments_to_save.clear()

    def video_recorder(self):
        cap = self._get_camera_capture()
        if not cap:
            self.speak("æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¡¬ä»¶è¿æ¥")
            return
        
        print("è§†é¢‘å½•åˆ¶å·²å¼€å§‹ï¼ˆå¤–æ¥æ‘„åƒå¤´ï¼‰")
        while self.is_listening and self.is_running:
            ret, frame = cap.read()
            if ret:
                # éé˜»å¡æ–¹å¼å†™å…¥é˜Ÿåˆ—ï¼Œé¿å…æ»¡é˜Ÿåˆ—é˜»å¡
                if not self.video_queue.full():
                    self.video_queue.put((frame, time.time()), block=False)
                self.video_buffer.append((frame, time.time()))
                cv2.imshow("USB Camera Feed", frame)  # è§†é¢‘çª—å£å USB Camera Feed
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            else:
                print("æ— æ³•è·å–æ‘„åƒå¤´ç”»é¢ï¼Œé‡è¯•ä¸­...")
                time.sleep(0.5)  # å‡å°‘é‡è¯•é¢‘ç‡ï¼Œé™ä½CPUå ç”¨

        cap.release()
        cv2.destroyAllWindows()
        print("è§†é¢‘å½•åˆ¶å·²åœæ­¢")

    # def check_vad_activity(self, audio_data):
    #     num, rate = 0, 0.6
    #     step = int(AUDIO_RATE * 0.02)
    #     flag_rate = round(rate * len(audio_data) // step)

    #     for i in range(0, len(audio_data), step):
    #         chunk = audio_data[i:i + step]
    #         if len(chunk) == step and self.vad.is_speech(chunk, AUDIO_RATE):
    #             num += 1
         
    #      # å¢åŠ èƒ½é‡æ£€æµ‹
    #     audio_np = np.frombuffer(audio_data, dtype=np.int16)
    #     energy = np.sqrt(np.mean(audio_np**2))

    #     return num > flag_rate

    def check_vad_activity(self, audio_data):
        # ==================== å‚æ•°è°ƒæ•´ï¼ˆæ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼‰ ====================
        # VADåˆ¤å®šå—æ•°æ¯”ä¾‹é˜ˆå€¼ï¼ˆåŸ0.4â†’0.6ï¼Œéœ€60%ä»¥ä¸Šçš„å—è¢«åˆ¤å®šä¸ºè¯­éŸ³ï¼‰
        THRESHOLD_RATIO = 0.5 
        # éŸ³é¢‘èƒ½é‡ç»å¯¹é˜ˆå€¼ï¼ˆæ ¹æ®ç¯å¢ƒè°ƒè¯•ï¼Œå€¼è¶Šå¤§è¶Šä¸¥æ ¼ï¼Œå…¸å‹å€¼1000~3000ï¼‰
        MIN_ENERGY = 10      
        # æ¯å—æ—¶é•¿ï¼ˆ20msï¼Œ48000Hzä¸‹æ¯å—960ä¸ªé‡‡æ ·ç‚¹ï¼‰
        STEP_MS = 0.02         
        step = int(AUDIO_RATE * STEP_MS)  # è®¡ç®—æ¯å—çš„é‡‡æ ·ç‚¹æ•°

        # ==================== è®¡ç®—æ€»å—æ•° ====================
        total_samples = len(audio_data)
        total_chunks = total_samples // step  # æ€»å—æ•°ï¼ˆå‘ä¸‹å–æ•´ï¼Œé¿å…ä¸å®Œæ•´å—ï¼‰
        if total_chunks == 0:
            return False  # éŸ³é¢‘æ•°æ®ä¸è¶³ä¸€ä¸ªå—ï¼Œæ— è¯­éŸ³æ´»åŠ¨

        # ==================== ç»Ÿè®¡VADè¯­éŸ³å—æ•° ====================
        speech_blocks = 0
        for i in range(total_chunks):
            start = i * step
            end = start + step
            chunk = audio_data[start:end]
            # ç¡®ä¿å—é•¿åº¦æ­£ç¡®ï¼ˆç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œå› total_chunkså·²å–æ•´ï¼‰
            if len(chunk) != step:
                continue
            # ä½¿ç”¨VADåˆ¤æ–­å½“å‰å—æ˜¯å¦ä¸ºè¯­éŸ³ï¼ˆä¾èµ–å·²è®¾ç½®çš„VAD_MODEï¼‰
            if self.vad.is_speech(chunk, AUDIO_RATE):
                speech_blocks += 1

        # ==================== è®¡ç®—èƒ½é‡ ====================
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è®¡ç®—å‡æ–¹æ ¹èƒ½é‡ï¼ˆåæ˜ æ•´ä½“éŸ³é‡ï¼‰
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        energy = np.sqrt(np.mean(audio_np ** 2))  # å‡æ–¹æ ¹èƒ½é‡
        # print(f"VADå—æ•°æ¯”ä¾‹: {speech_blocks} | èƒ½é‡å€¼: {energy} | é˜ˆå€¼: {MIN_ENERGY}")
        # ==================== ç»¼åˆåˆ¤æ–­ ====================
        # æ¡ä»¶1ï¼šè¯­éŸ³å—æ•°è¶…è¿‡é˜ˆå€¼æ¯”ä¾‹ï¼ˆå¦‚60%ï¼‰
        # æ¡ä»¶2ï¼šèƒ½é‡è¶…è¿‡ç»å¯¹é˜ˆå€¼ï¼ˆè¿‡æ»¤ä½éŸ³é‡å™ªå£°ï¼‰
        return speech_blocks > int(total_chunks * THRESHOLD_RATIO) and energy > MIN_ENERGY
    
    def inference(self, video_frames, audio_data):
        """å†…å­˜ä¸­æ¨ç†ï¼ˆå»é™¤æ–‡ä»¶æ“ä½œï¼ŒæŒ‡ä»¤åˆ†ç±»ä¸ç›®æ ‡/å¯¼èˆªéœ€æ±‚è§£æï¼‰"""
        # æå–å…³é”®å¸§
        key_frames = []
        if video_frames:
            total_frames = len(video_frames)
            # å–3å¸§ï¼šå‰1/4ã€ä¸­é—´ã€å1/4ï¼ˆè¦†ç›–ä¸åŒè§’åº¦ï¼‰
            frame_indices = [
                int(total_frames * 0.25),
                int(total_frames * 0.5),
                int(total_frames * 0.75)
            ]
            for idx in frame_indices:
                if 0 <= idx < total_frames:
                    frame = video_frames[idx]
                    pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                    key_frames.append(pil_frame)
        
        # è¯­éŸ³è¯†åˆ«ï¼ˆç›´æ¥ä»å†…å­˜æ•°æ®ï¼‰
        try:
            result = self.sensevoice_pipeline(audio_data, audio_fs=16000)
            raw_text = result[0].get("text", "").strip() if isinstance(result, list) else result.get("text", "")
            raw_text = re.sub(r'<\|.*?\|>|[^\u4e00-\u9fa5ï¼Œã€‚ï¼Ÿï¼\s]', '', raw_text)
            
            if not raw_text or len(raw_text) < 2:
                print(f"æ— æ•ˆè¯­éŸ³è¾“å…¥: {raw_text}")
                return
                
            print(f"è¯­éŸ³è¯†åˆ«ç»“æœ: {raw_text}")
            # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºå¯¼èˆªåŒ¹é…
            self.last_nav_query = raw_text

        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
            self.speak("è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•")
            return

        # å¤„ç†è¯­éŸ³æŒ‡ä»¤ï¼ˆèåˆåˆ†ç±»ä¸è¯­ä¹‰è§£æï¼‰
        try:
            prompt = (
                f"è¯·åˆ†æä»¥ä¸‹è¯­éŸ³æŒ‡ä»¤ï¼Œå®Œæˆä¸¤é¡¹ä»»åŠ¡å¹¶ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡ºï¼š\n"
                f"è¯­éŸ³æŒ‡ä»¤ï¼š'{raw_text}'\n"
                f"ä»»åŠ¡1ï¼šæŒ‡ä»¤åˆ†ç±»\n"
                f" - åˆ†ç±»ç±»å‹ï¼šå¯¼èˆªã€å›¾åƒè¯†åˆ«ã€å¯¹è¯ã€æ·»åŠ ä»»åŠ¡ç‚¹ã€ä¿®æ”¹ä»»åŠ¡ç‚¹ã€é—®å€™\n"
                f" - åˆ†ç±»è§„åˆ™ï¼š\n"
                f"   - å«'å»'ã€'åˆ°'ã€'å¯¼èˆª'ã€'æ‹¿'ã€'å–'ã€'æ‰¾'â†’å¯¼èˆªï¼›\n"
                f"   - å«'è¯†åˆ«'ã€'çœ‹'ã€'æ£€æµ‹'â†’å›¾åƒè¯†åˆ«ï¼›\n"
                f"   - å«'æ·»åŠ 'ã€'å¢åŠ 'ã€'ä¿å­˜'ã€'è®°å½•'â†’æ·»åŠ ä»»åŠ¡ç‚¹ï¼›\n"
                f"   - å«'ä¿®æ”¹'ã€'è®¾å®šä¸º'â†’ä¿®æ”¹ä»»åŠ¡ç‚¹ï¼›\n"
                f"   - å«'é—®å€™'ã€'æ‹›å‘¼'ã€'é—®å¥½'â†’é—®å€™ï¼›\n"
                f"   - æ— ä¸Šè¿°å…³é”®è¯â†’å¯¹è¯\n"
                f"   - éœ€æå–åˆ†ç±»ä¾æ®çš„å…³é”®è¯ï¼ˆä»åŸå§‹è¯­éŸ³ä¸­æå–ï¼‰\n"

                f"ä»»åŠ¡2ï¼šè¯­ä¹‰è§£æ\n"
                f" - æå–targetï¼šç”¨æˆ·æƒ³æ‰¾/æ‹¿/å–çš„æ ¸å¿ƒç‰©å“ï¼ˆæ— åˆ™ä¸ºç©ºï¼‰ï¼›\n"
                f" - åˆ¤æ–­need_navigateï¼šæ˜¯å¦éœ€è¦å¯¼èˆªï¼ˆtrue/falseï¼‰ï¼›\n"
                f" - è¯´æ˜reasonï¼šåˆ†ç±»åŠå¯¼èˆªåˆ¤æ–­çš„ç»¼åˆä¾æ®\n"
                f"è¾“å‡ºæ ¼å¼ï¼ˆä»…JSONï¼Œæ— å¤šä½™æ–‡æœ¬ï¼‰ï¼š\n"
                f'{{"type": "æŒ‡ä»¤ç±»å‹", "keyword": "åˆ†ç±»å…³é”®è¯", "target": "ç›®æ ‡ç‰©å“", "need_navigate": true/false, "reason": "ç»¼åˆåˆ¤æ–­ä¾æ®"}}\n'
                f"ç¤ºä¾‹ï¼š\n"
                f'æŒ‡ä»¤ï¼š"å»æ‹¿æ¡Œå­ä¸Šçš„æ°´ç“¶" â†’ è¾“å‡ºï¼š{{"type": "å¯¼èˆª", "keyword": "å»ã€æ‹¿", "target": "æ°´ç“¶", "need_navigate": true, "reason": "å«\'å»ã€æ‹¿\'å…³é”®è¯å±äºå¯¼èˆªï¼Œéœ€æ‹¿æ°´ç“¶æ•…éœ€è¦å¯¼èˆª"}}\n'
                f'æŒ‡ä»¤ï¼š"è¯†åˆ«ä¸€ä¸‹è¿™æ˜¯ä»€ä¹ˆ" â†’ è¾“å‡ºï¼š{{"type": "å›¾åƒè¯†åˆ«", "keyword": "è¯†åˆ«", "target": "", "need_navigate": false, "reason": "å«\'è¯†åˆ«\'å…³é”®è¯å±äºå›¾åƒè¯†åˆ«ï¼Œä¸å¯¼èˆªæ— å…³"}}\n'
                f'æŒ‡ä»¤ï¼š"ä»Šå¤©å¤©æ°”å¦‚ä½•" â†’ è¾“å‡ºï¼š{{"type": "å¯¹è¯", "keyword": "", "target": "", "need_navigate": false, "reason": "æ— ç‰¹å®šå…³é”®è¯å±äºå¯¹è¯ï¼Œä¸å¯¼èˆªæ— å…³"}}'
            )

            # æ„é€ åƒé—®æ¨¡å‹çš„è¾“å…¥ï¼ˆåŒ…å«å…³é”®å¸§è¾…åŠ©ç†è§£ï¼‰
            messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
            if key_frames:
                # æ’å…¥æ‰€æœ‰å…³é”®å¸§è¾…åŠ©åœºæ™¯ç†è§£
                for i, frame in enumerate(key_frames):
                    messages[0]["content"].insert(i, {"type": "image", "image": frame})

            # è°ƒç”¨åƒé—®æ¨¡å‹è§£æ
            text = self.qwen_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages) if key_frames else (None, None)
            
            inputs = self.qwen_processor(
                text=[text], 
                images=image_inputs, 
                padding=True, 
                return_tensors="pt"
            ).to(self.device)

            generated_ids = self.qwen_model.generate(**inputs, max_new_tokens=200)
            generated_ids_trimmed = generated_ids[0][len(inputs['input_ids'][0]):]
            combined_result = self.qwen_processor.decode(generated_ids_trimmed, skip_special_tokens=True).strip()
            
            print(f"èåˆè§£æç»“æœ: {combined_result}")
            # å¤„ç†èåˆç»“æœï¼ˆæ•´åˆåŸä¸¤ç§å¤„ç†é€»è¾‘ï¼‰
            self.process_combined_result(combined_result, raw_text)

        except Exception as e:
            print(f"æŒ‡ä»¤è§£æé”™è¯¯: {e}")
            self.speak("æœªèƒ½ç†è§£æ‚¨çš„æŒ‡ä»¤ï¼Œè¯·é‡æ–°è¡¨è¿°")

    def process_combined_result(self, combined_result, raw_text):
        """
        å¤„ç†èåˆåçš„æŒ‡ä»¤æŒ‡ä»¤è§£æç»“æœ
        combined_result: æ¨¡å‹è¿”å›çš„èåˆè§£æç»“æœï¼ˆJSONå­—ç¬¦ä¸²ï¼‰
        raw_text: åŸå§‹å§‹è¯­éŸ³è¯†åˆ«æ–‡æœ¬
        """
        # æ¸…ç†æ¨¡å‹è¾“å‡ºæ ¼å¼ï¼ˆç§»é™¤ä»£ç å—æ ‡è®°å’Œæ— å…³å­—ç¬¦ï¼‰
        cleaned_result = re.sub(r'^```json\s*', '', combined_result)
        cleaned_result = re.sub(r'\s*```$', '', cleaned_result)
        # è¡¥å……æ·»åŠ å¯¹ä¸‹åˆ’çº¿_çš„æ”¯æŒï¼Œç¡®ä¿need_navigateç­‰å­—æ®µæ­£ç¡®ä¿ç•™
        cleaned_result = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\{\}\"\:\,\s\_truefalse]', '', cleaned_result).strip()
        print(f"æ¸…ç†åçš„èåˆç»“æœ: {cleaned_result}")

        try:
            # è§£æJSONå¹¶æ ¡éªŒå¿…è¦å­—æ®µ
            result = json.loads(cleaned_result)
            
            # æå–æ ¸å¿ƒå‚æ•°ï¼Œå¢åŠ é»˜è®¤å€¼å¤„ç†
            cmd_type = result.get("type", "").strip()
            keyword = result.get("keyword", "").strip()
            target = result.get("target", "").strip()
            need_navigate = result.get("need_navigate", False)
            reason = result.get("reason", "æ— è¯´æ˜")

            # æ ¡éªŒæŒ‡ä»¤ç±»å‹åˆæ³•æ€§ï¼Œå¢åŠ å®¹é”™å¤„ç†
            valid_types = ["å¯¼èˆª", "å›¾åƒè¯†åˆ«", "å¯¹è¯", "æ·»åŠ ä»»åŠ¡ç‚¹", "ä¿®æ”¹ä»»åŠ¡ç‚¹", "é—®å€™"]
            if cmd_type not in valid_types:
                print(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆæŒ‡ä»¤ç±»å‹: {cmd_type}ï¼Œå°è¯•è‡ªåŠ¨æ¨æ–­...")
                
                # è‡ªåŠ¨æ¨æ–­ç±»å‹
                if any(kw in raw_text for kw in ["å»", "åˆ°", "å¯¼èˆª", "æ‹¿", "å–", "æ‰¾"]):
                    cmd_type = "å¯¼èˆª"
                elif any(kw in raw_text for kw in ["è¯†åˆ«", "çœ‹", "æ£€æµ‹"]):
                    cmd_type = "å›¾åƒè¯†åˆ«"
                elif any(kw in raw_text for kw in ["æ·»åŠ ", "å¢åŠ ", "ä¿å­˜", "è®°å½•"]):
                    cmd_type = "æ·»åŠ ä»»åŠ¡ç‚¹"
                elif any(kw in raw_text for kw in ["ä¿®æ”¹", "è®¾å®šä¸º"]):
                    cmd_type = "ä¿®æ”¹ä»»åŠ¡ç‚¹"
                elif any(kw in raw_text for kw in ["é—®å€™", "æ‹›å‘¼", "é—®å¥½"]):
                    cmd_type = "é—®å€™"
                else:
                    cmd_type = "å¯¹è¯"
                    
                print(f"è‡ªåŠ¨æ¨æ–­æŒ‡ä»¤ç±»å‹ä¸º: {cmd_type}")

            print(f"èåˆè§£æè¯¦æƒ…ï¼šç±»å‹={cmd_type}ï¼Œå…³é”®è¯={keyword}ï¼Œç›®æ ‡={target}ï¼Œéœ€å¯¼èˆª={need_navigate}ï¼Œä¾æ®={reason}")

        except (json.JSONDecodeError, ValueError) as e:
            print(f"èåˆç»“æœè§£æé”™è¯¯: {e}ï¼ŒåŸå§‹è¾“å‡º: {combined_result}")
            # å°è¯•ç›´æ¥æ ¹æ®åŸå§‹æ–‡æœ¬æ¨æ–­
            print("å°è¯•ç›´æ¥æ ¹æ®åŸå§‹æ–‡æœ¬æ¨æ–­æŒ‡ä»¤ç±»å‹...")
            if any(kw in raw_text for kw in ["å»", "åˆ°", "å¯¼èˆª", "æ‹¿", "å–", "æ‰¾"]):
                cmd_type = "å¯¼èˆª"
                target = raw_text
                need_navigate = True
            elif any(kw in raw_text for kw in ["è¯†åˆ«", "çœ‹", "æ£€æµ‹"]):
                cmd_type = "å›¾åƒè¯†åˆ«"
                target = ""
                need_navigate = False
            elif any(kw in raw_text for kw in ["æ·»åŠ ", "å¢åŠ ", "ä¿å­˜", "è®°å½•"]):
                cmd_type = "æ·»åŠ ä»»åŠ¡ç‚¹"
                target = ""
                need_navigate = False
            elif any(kw in raw_text for kw in ["ä¿®æ”¹", "è®¾å®šä¸º"]):
                cmd_type = "ä¿®æ”¹ä»»åŠ¡ç‚¹"
                target = ""
                need_navigate = False
            elif any(kw in raw_text for kw in ["é—®å€™", "æ‹›å‘¼", "é—®å¥½"]):
                cmd_type = "é—®å€™"
                target = ""
                need_navigate = False
            else:
                cmd_type = "å¯¹è¯"
                target = raw_text
                need_navigate = False
                
            print(f"ä»åŸå§‹æ–‡æœ¬æ¨æ–­ï¼šç±»å‹={cmd_type}ï¼Œç›®æ ‡={target}")
            keyword = ""
            reason = "è§£æå¤±è´¥åä»åŸå§‹æ–‡æœ¬æ¨æ–­"

        # æ•´åˆç±»å‹ä¿®æ­£é€»è¾‘ï¼ˆä¼˜å…ˆäºæ¨¡å‹åˆ¤æ–­ï¼‰
        image_keywords = {"æ£€æµ‹", "è¯†åˆ«", "å›¾åƒ", "ç›®æ ‡æ£€æµ‹", "æ‘„åƒå¤´", "çœ‹", "åˆ†æ"}
        has_image_kw = any(kw in raw_text for kw in image_keywords)
        if has_image_kw and cmd_type != "å›¾åƒè¯†åˆ«":
            print(f"å¼ºåˆ¶ä¿®æ­£ç±»å‹ï¼š{cmd_type} â†’ å›¾åƒè¯†åˆ«ï¼ˆåŸå§‹è¯­éŸ³å«å›¾åƒå…³é”®è¯ï¼‰")
            cmd_type = "å›¾åƒè¯†åˆ«"

        # å…³é”®è¯ç›´æ¥åŒ¹é…å›é€€é€»è¾‘ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        for kw, cmd in self.command_mapping.items():
            if kw in raw_text:
                print(f"è§¦å‘å…³é”®è¯ç›´æ¥åŒ¹é…ï¼š{kw} â†’ {cmd}")
                self.command_queue.put((cmd, kw))
                return

        # æŒ‰æŒ‡ä»¤ç±»å‹å¤„ç†ï¼ˆæ•´åˆåˆ†ç±»ä¸è¯­ä¹‰è§£æç»“æœï¼‰
        try:
            if cmd_type == "å¯¼èˆª":
                # ç»“åˆè¯­ä¹‰è§£æçš„targetå’Œneed_navigateå¤„ç†å¯¼èˆª
                if need_navigate and target:
                    # ä¼˜å…ˆç”¨è§£æå‡ºçš„targetåŒ¹é…ä»»åŠ¡ç‚¹
                    print(f"å°è¯•ç”¨ç›®æ ‡ç‰©å“'{target}'åŒ¹é…ä»»åŠ¡ç‚¹...")
                    matched_point = self.find_point_by_environment(target)
                    if matched_point:
                        # ç›´æ¥ä½¿ç”¨åŒ¹é…åˆ°çš„ä»»åŠ¡ç‚¹IDå¯¼èˆªï¼Œç›´æ¥è°ƒç”¨send_goal_pointæ–¹æ³•
                        print(f"æ‰¾åˆ°åŒ¹é…ä»»åŠ¡ç‚¹: {matched_point}ï¼Œç›´æ¥å¯¼èˆª")
                        # å¢åŠ ä»»åŠ¡ç‚¹æè¿°çš„è¯­éŸ³åé¦ˆ
                        desc = self.task_points.get(matched_point, {}).get("environment", "æœªçŸ¥ä½ç½®")
                        self.speak(f"æ­£åœ¨å¯¼èˆªåˆ°{target}æ‰€åœ¨ä½ç½®ï¼ˆä»»åŠ¡ç‚¹{matched_point}ï¼Œ{desc}ï¼‰")
                        # ç›´æ¥è°ƒç”¨å‡½æ•°å‘é€ç›®æ ‡ç‚¹
                        self.send_goal_point(matched_point)
                        return
                    else:
                        self.speak(f"æœªæ‰¾åˆ°åŒ…å«'{target}'çš„ä»»åŠ¡ç‚¹ï¼Œå°†æŒ‰åŸå§‹æŒ‡ä»¤æœç´¢")
                
                # æœªåŒ¹é…åˆ°ç›®æ ‡æ—¶ï¼Œç”¨åŸå§‹æ–‡æœ¬å¯¼èˆª
                self.command_queue.put((CommandType.NAVIGATE, raw_text))

            elif cmd_type == "å›¾åƒè¯†åˆ«":
                self.command_queue.put((CommandType.IMAGE_RECOGNITION, keyword or raw_text))

            elif cmd_type == "å¯¹è¯":
                # ç»“åˆtargetæä¾›æ›´ç²¾å‡†çš„å¯¹è¯ä¸Šä¸‹æ–‡
                dialogue_content = f"{target}ç›¸å…³å†…å®¹ï¼š{raw_text}" if target else raw_text
                self.command_queue.put((CommandType.DIALOGUE, dialogue_content))

            elif cmd_type == "æ·»åŠ ä»»åŠ¡ç‚¹":
                # ç”¨targetä½œä¸ºä»»åŠ¡ç‚¹æè¿°ï¼ˆè‹¥æœ‰ï¼‰
                point_desc = target if target else keyword
                self.command_queue.put((CommandType.ADD_TASK_POINT, point_desc))

            elif cmd_type == "ä¿®æ”¹ä»»åŠ¡ç‚¹":
                # ä»resultæå–point_idï¼Œé»˜è®¤0
                point_id = result.get("point_id", 0) if isinstance(result, dict) else 0
                # ç”¨targetä½œä¸ºä¿®æ”¹åçš„æè¿°ï¼ˆè‹¥æœ‰ï¼‰
                new_desc = target if target else keyword
                self.command_queue.put(((CommandType.MODIFY_TASK_POINT, point_id), new_desc))

            elif cmd_type == "é—®å€™":
                self.command_queue.put((CommandType.GREET, 9))  # 9ä¸ºé—®å€™æŒ‡ä»¤å‚æ•°

        except Exception as e:
            print(f"èåˆç»“æœå¤„ç†é€»è¾‘é”™è¯¯: {e}")
            self.speak("å¤„ç†æŒ‡ä»¤æ—¶å‡ºé”™ï¼Œè¯·é‡è¯•")
    

    def _process_commands(self):
        """å¤„ç†å‘½ä»¤é˜Ÿåˆ—ä¸­çš„æŒ‡ä»¤"""
        while self.is_running:
            try:
                command = self.command_queue.get(timeout=1)
                # è§£ææŒ‡ä»¤ï¼šå¯èƒ½æ˜¯ (ç±»å‹, å‚æ•°) æˆ– ((ç±»å‹, å­å‚æ•°), å‚æ•°)
                cmd_code, param = command[0], command[1] 
                # å¤„ç†å…ƒç»„ç±»å‹çš„æŒ‡ä»¤ï¼ˆå¦‚ MODIFY_TASK_POINT åŒ…å«å­å‚æ•°ï¼‰
                if isinstance(cmd_code, tuple):
                    cmd_type, sub_param = cmd_code  # æ‹†åˆ†å…ƒç»„ï¼š(8, 0) â†’ ç±»å‹8ï¼Œå­å‚æ•°0
                    print(f"ğŸ“Œ è§£æåˆ°å…ƒç»„æŒ‡ä»¤ï¼šç±»å‹={cmd_type}ï¼Œå­å‚æ•°={sub_param}ï¼Œå‚æ•°={param}")
                else:
                    cmd_type = cmd_code  # éå…ƒç»„ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç±»å‹
                    sub_param = None  # æ— é¢å¤–å­å‚æ•°
                    print(f"è§£æåˆ°æ™®é€šæŒ‡ä»¤ï¼šç±»å‹={cmd_type}ï¼Œå‚æ•°={param}")
                
                self.speak(f"æ”¶åˆ°æŒ‡ä»¤: {param}")

                # ç”¨ cmd_type è¿›è¡Œåˆ¤æ–­
                if cmd_type == CommandType.ADD_TASK_POINT:
                    self.add_task_point()
                elif cmd_type == CommandType.MODIFY_TASK_POINT:
                    print(f"ğŸ” å‡†å¤‡è°ƒç”¨modify_task_pointï¼Œå‚æ•°point_id={sub_param}")
                    self.modify_task_point(sub_param)  # è°ƒç”¨ä¿®æ”¹ä»»åŠ¡ç‚¹å‡½æ•°
                    print(f"modify_task_pointè°ƒç”¨å®Œæˆï¼Œå‚æ•°point_id={sub_param}")

                # elif cmd_type == CommandType.IMAGE_RECOGNITION:
                #     self.process_image_recognition()
                # elif cmd_type == CommandType.DIALOGUE:
                #     self.process_dialogue(param)
                # elif cmd_type == CommandType.GREET:
                #     self.send_lerobot(int(sub_param))
                # elif cmd_type == CommandType.NAVIGATE:
                #     # å¤„ç†å¯¼èˆªæŒ‡ä»¤ï¼Œåˆå¹¶äº†æŒ‰IDå’ŒæŒ‰æè¿°å¯¼èˆª
                #     # å¦‚æœsub_paramå­˜åœ¨ï¼Œè¯´æ˜æ˜¯ç‰¹å®šä»»åŠ¡ç‚¹IDå¯¼èˆª
                #     # å¦åˆ™ä½¿ç”¨paramä½œä¸ºæŸ¥è¯¢æ–‡æœ¬è¿›è¡ŒæŒ‰æè¿°å¯¼èˆª
                #     self.handle_navigation(command_code=sub_param, query_text=param)
                else:
                    print(f"æœªçŸ¥æŒ‡ä»¤ç±»å‹: {cmd_type}")
                    self.speak("æœªçŸ¥æŒ‡ä»¤ï¼Œè¯·é‡è¯•")
                
                self.command_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                print(f"å‘½ä»¤å¤„ç†é”™è¯¯: {e}")
                self.speak("å¤„ç†æŒ‡ä»¤æ—¶å‡ºé”™ï¼Œè¯·é‡è¯•")

    def stop_system(self):
            """å®Œå–„çš„èµ„æºé‡Šæ”¾é€»è¾‘"""
            print("å¼€å§‹å…³é—­ç³»ç»Ÿ...")
            # åœæ­¢æ ‡å¿—
            self.is_listening = False
            self.is_running = False
            self.recording_active = False
            
            # åœæ­¢éŸ³é¢‘æ’­æ”¾
            if pygame.mixer.music.get_busy():
                pygame.mixer.music.stop()
            
            # å…³é—­MQTTè¿æ¥
            if self.mqtt_client:
                self.mqtt_client.loop_stop()
                self.mqtt_client.disconnect()
            
            # å…³é—­éŸ³é¢‘æµ
            if self.audio_stream:
                self.audio_stream.stop_stream()
                self.audio_stream.close()
            
            # ç»ˆæ­¢PyAudio
            if self.pyaudio_instance:
                self.pyaudio_instance.terminate()
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if hasattr(self, 'audio_thread') and self.audio_thread.is_alive():
                self.audio_thread.join(timeout=2)
                print(f"éŸ³é¢‘çº¿ç¨‹å·²{'æ­£å¸¸' if not self.audio_thread.is_alive() else 'å¼ºåˆ¶'}ç»“æŸ")
            
            if hasattr(self, 'video_thread') and self.video_thread.is_alive():
                self.video_thread.join(timeout=2)
                print(f"è§†é¢‘çº¿ç¨‹å·²{'æ­£å¸¸' if not self.video_thread.is_alive() else 'å¼ºåˆ¶'}ç»“æŸ")
            
            if hasattr(self, 'command_thread') and self.command_thread.is_alive():
                self.command_thread.join(timeout=2)
                print(f"å‘½ä»¤å¤„ç†çº¿ç¨‹å·²{'æ­£å¸¸' if not self.command_thread.is_alive() else 'å¼ºåˆ¶'}ç»“æŸ")
            
            # æ¸…ç†CUDAå†…å­˜
            if self.device and self.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            print("ç³»ç»Ÿå·²å®Œå…¨å…³é—­")


    def run(self):
        try:
            self.start_listening()
            while self.is_running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            self.speak("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œå†è§")
            time.sleep(1)
        finally:
            self.stop_system()

def main():
    nav_system = VoiceNavigationSystem()
    nav_system.run()

if __name__ == "__main__":
    main()
